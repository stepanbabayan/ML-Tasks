{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aced9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9f48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2800e489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(age                     41\n",
       " ed          college degree\n",
       " employ                  17\n",
       " address                 12\n",
       " income                 176\n",
       " debtinc                9.3\n",
       " creddebt         11.359392\n",
       " othdebt           5.008608\n",
       " default                  1\n",
       " Name: 0, dtype: object,\n",
       " (9,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0], df.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b95d008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nh_beta(X) = 1 / (1 + e**(-beta.dot(X)))    # Prediction\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "h_beta(X) = 1 / (1 + e**(-beta.dot(X)))    # Prediction\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906a6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression1:\n",
    "    \"\"\"\n",
    "    Implement logistic regression with GD.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.1, max_iter=3000, metric_change=0.000001):\n",
    "        \"\"\"\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        self._lr = lr\n",
    "        self._weights = None\n",
    "        self.max_iter = max_iter\n",
    "        self.metric_change = metric_change\n",
    "\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "\n",
    "\n",
    "    def loss_function(self):\n",
    "        \"\"\"\n",
    "        TODO: calculate the loss function\n",
    "        \"\"\"\n",
    "        if self._x is None or self._y is None:\n",
    "            raise ValueError('All methods can be called after fit method is called.')\n",
    "        _loss = np.sum((1 - self._y) * np.log(1 - self.sigmoid(self._x)) + self._y * np.log(self.sigmoid(self._x)))\n",
    "        return _loss\n",
    "        \n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        TODO: Calculate the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        grad = []\n",
    "        for i in range(x.shape[1]):\n",
    "            grad.append(np.sum(x[:,i]*(self.sigmoid(x) - y)))\n",
    "        \n",
    "        return np.array(grad)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        :param x: feature matrix.\n",
    "        :returns: sigmoid vector of all features.\n",
    "        TODO: Add ones to feature matrix and compute sigmoid.\n",
    "        \"\"\"\n",
    "        if self._weights is None:\n",
    "            raise ValueError('All methods can be called after fit method is called.')\n",
    "\n",
    "        _sigmoid = 1 / (1 + np.exp(-x.dot(self._weights.T)))\n",
    "\n",
    "        return _sigmoid\n",
    "    \n",
    "    def update_weights(self):\n",
    "        grad = self.gradient(self._x, self._y)\n",
    "        self._weights = self._weights - self._lr * grad\n",
    "        self.coef_ = [self._weights[1:]]\n",
    "        self.intercept_ = self._weights[0]\n",
    "        return\n",
    "\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        TODO: normalize the data and fit the logistic regression.\n",
    "        :param x: features matrix\n",
    "        :param y: labels\n",
    "        :returns: None if can't fit, weights, if fitted.\n",
    "        \"\"\"\n",
    "        # TODO: initialize weights here\n",
    "        l = x.shape[0]\n",
    "        x = np.concatenate([np.ones(l).reshape((-1, 1)), x], axis=1)\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._weights = np.zeros(self._x.shape[1])\n",
    "\n",
    "        # TODO: SGD code here\n",
    "        self.L = self.loss_function()\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            self.update_weights()\n",
    "            temp_change = np.abs(self.L - self.loss_function())\n",
    "            self.L = self.loss_function()\n",
    "\n",
    "            if temp_change < self.metric_change:\n",
    "                break\n",
    "\n",
    "        return self._weights\n",
    "\n",
    "    def predict(self, x, threshold=.5):\n",
    "        \"\"\"\n",
    "        Predict which class is each data in x\n",
    "        :param x: features matrix\n",
    "        \"\"\"\n",
    "        l = x.shape[0]\n",
    "        x = np.concatenate([np.ones(l).reshape((-1, 1)), x], axis=1)\n",
    "        return np.where(self.predict_proba(x) >= threshold, 1, 0)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Predict the probability, that x is of class 1.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def perf_measure(self, y_pred, y):\n",
    "        a = np.unique(y_pred + y, return_counts=True)\n",
    "        b = np.unique(y_pred - y, return_counts=True)\n",
    "        print(a)\n",
    "        print(b)\n",
    "        TP = a[1][2]\n",
    "        TN = a[1][0]\n",
    "        FP = b[1][2]\n",
    "        FN = b[1][0]\n",
    "        print(TP, TN, FP, FN)\n",
    "        return (TP, FP, TN, FN)\n",
    "    \n",
    "    def calculate_metrics(self, tp, fp, tn, fn):\n",
    "#         Following 4 metrics should be high so the model is good.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        npv = tn / (tn + fn)\n",
    "    \n",
    "        accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        print(\"Precision:                \", precision)\n",
    "        print(\"Recall:                   \", recall)\n",
    "        print(\"Specificity:              \", specificity)\n",
    "        print(\"Negative predictive value:\", npv)\n",
    "        print(\"Accuracy:                 \", accuracy)\n",
    "        print(\"f1_score:                 \", f1_score)\n",
    "\n",
    "        return (accuracy, f1_score)\n",
    "\n",
    "    def score(self, x_test, y):\n",
    "        \"\"\"\n",
    "        TODO: Compute the accuracy and f1 score here.\n",
    "        \"\"\"\n",
    "        tp, fp, tn, fn = self.perf_measure(self.predict(x_test), y)\n",
    "        \n",
    "        return self.calculate_metrics(tp, fp, tn, fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2def40b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2], dtype=int64), array([97, 31, 12], dtype=int64))\n",
      "(array([-1,  0,  1], dtype=int64), array([ 27, 109,   4], dtype=int64))\n",
      "12 97 4 27\n",
      "Precision:                 0.75\n",
      "Recall:                    0.3076923076923077\n",
      "Specificity:               0.9603960396039604\n",
      "Negative predictive value: 0.782258064516129\n",
      "Accuracy:                  0.7785714285714286\n",
      "f1_score:                  0.4363636363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7642857142857142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('classification.csv')\n",
    "x = df.iloc[:, 5:7].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=0)\n",
    "\n",
    "model = LogisticRegression1(lr=0.01, max_iter=5000)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# TODO: Calculate metrics here.\n",
    "scores = model.score(x_test, y_test)\n",
    "\n",
    "\n",
    "# TODO: Compare your results with sklearn.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68505505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([4.87338254, 1.71892885])], -2.4941478302197417)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_, model.intercept_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b83399e872f06067aea4ee1acc358d7c36a8ef71a15f614d2a73338f2c0644b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
